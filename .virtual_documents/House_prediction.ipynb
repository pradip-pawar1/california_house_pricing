import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv("housing.csv")





df.head()


df.tail()


df.info()


df.shape


df.describe()


df.columns


columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income',
       'median_house_value']

for cols in columns:
    plt.figure(figsize=(6,4))
    sns.histplot(df[cols], kde = True, bins = 20)


df.head()


df.duplicated().sum()


# House prices
sns.histplot(df['median_house_value'], bins=50, kde=True)
plt.show()

# Median income
sns.histplot(df['median_income'], bins=50, kde=True)
plt.show()



# Removing `median_house_value` which are grater than 500,000
df = df[df['median_house_value'] < 500000]


df.shape





plt.figure(figsize=(10,6))
plt.scatter(df['longitude'], df['latitude'], alpha=0.4,
            c=df['median_house_value'], cmap='viridis')
plt.colorbar(label="House Value")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()


plt.figure(figsize=(10,6))
plt.scatter(df['longitude'], df['latitude'], alpha=0.4,
            s=df['population']/100,  # size scaled by population
            c=df['median_house_value'], cmap='viridis')
plt.colorbar(label="House Value")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()






df.head()


# correlation matrix
numeric_df = df.select_dtypes(include=['int64', 'float64'])
corr_matrix = numeric_df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap", fontsize=16)
plt.show()


df_cleaned = df.copy()


df_cleaned["rooms_per_household"] = df_cleaned["total_rooms"] / df_cleaned["households"]
df_cleaned["bedrooms_per_room"] = df_cleaned["total_bedrooms"] / df_cleaned["total_rooms"]
df_cleaned["population_per_household"] = df_cleaned["population"] / df_cleaned["households"]






# 1. One-hot encode `ocean_proximity`
df_cleaned = pd.get_dummies(df_cleaned, columns=["ocean_proximity"], drop_first=True)


bool_cols = ['ocean_proximity_INLAND', 
             'ocean_proximity_ISLAND',	
             'ocean_proximity_NEAR BAY',	
             'ocean_proximity_NEAR OCEAN']

df_cleaned[bool_cols] = df_cleaned[bool_cols].astype(int)


# 2. Droping highly correlated / redundant columns
cols_to_drop = ["total_rooms", "total_bedrooms", "population", "households"]
df_cleaned = df_cleaned.drop(columns=cols_to_drop)


# 3. Check correlation with target again
corr_matrix = df_cleaned.corr()
print(corr_matrix["median_house_value"].sort_values(ascending=False))


# 4. Your final dataset is ready
print("Final shape:", df_cleaned.shape)
print("Final columns:", df_cleaned.columns.tolist())


df_cleaned.isnull().sum()


df_cleaned["bedrooms_per_room"] = df_cleaned["bedrooms_per_room"].fillna(0)






from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df_cleaned.drop("median_house_value", axis=1)
y = df_cleaned["median_house_value"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


from xgboost import XGBRegressor
import numpy as np


model = XGBRegressor()


model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)


rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("R2 Score:", r2)



